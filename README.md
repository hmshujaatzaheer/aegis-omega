# AEGIS-Î©: Universal AI Safety Protocol

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> **The TCP/IP of AI Safety Infrastructure**

AEGIS-Î© (Autonomous Enforcement through Guaranteed Intelligent Specification - Omega) is the world's first formally verified, compositional AI safety protocol stack. Just as TCP/IP enabled the Internet by providing universal communication protocols, AEGIS-Î© enables the AI safety ecosystem by providing universal safety verification protocols.

## ðŸŒ Vision: Infrastructure for Humanity's AI Future

AEGIS-Î© addresses a fundamental gap in AI safety: **the absence of universal, mathematically guaranteed safety infrastructure**. Current approaches are:

- **Fragmented**: Each AI system implements ad-hoc safety measures
- **Unverifiable**: No mathematical proofs of safety properties
- **Incompatible**: AI systems cannot verify each other's safety
- **Non-composable**: Safety doesn't propagate through pipelines

AEGIS-Î© solves these problems with four revolutionary contributions that work together as a unified protocol stack.

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AEGIS-Î© Protocol Stack                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 4: Safety Handshake Protocol                             â”‚
â”‚  â”œâ”€â”€ AI-to-AI safety negotiation                                â”‚
â”‚  â”œâ”€â”€ Certificate exchange                                       â”‚
â”‚  â””â”€â”€ Continuous monitoring                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: Folded-ZKML                                           â”‚
â”‚  â”œâ”€â”€ Zero-knowledge safety proofs                               â”‚
â”‚  â”œâ”€â”€ Nova IVC folding                                           â”‚
â”‚  â””â”€â”€ Privacy-preserving compliance                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Categorical Safety Composition                        â”‚
â”‚  â”œâ”€â”€ Safety Category framework                                  â”‚
â”‚  â”œâ”€â”€ Certificate Functor                                        â”‚
â”‚  â””â”€â”€ Pipeline composition with guarantees                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Streaming-MFOTL                                       â”‚
â”‚  â”œâ”€â”€ Bounded-memory verification                                â”‚
â”‚  â”œâ”€â”€ Real-time monitoring                                       â”‚
â”‚  â””â”€â”€ EU AI Act formalization                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“š Four Revolutionary Contributions

### 1. Streaming-MFOTL: Bounded-Memory Runtime Verification

**Problem**: Traditional MFOTL verification requires unbounded memory for infinite traces.

**Solution**: Novel streaming algorithm with O(B Ã— |Ï†|^d) memory, independent of trace length.

```python
from aegis_omega import StreamingMFOTLMonitor, Event, Predicate, Always

# Create monitor with bounded memory
monitor = StreamingMFOTLMonitor(
    formula=Always(Predicate("action_logged"), window=1000),
    window_size=1000,
    memory_budget_mb=100
)

# Process infinite event stream with constant memory
for event in event_stream:
    verdict = monitor.process_event(event)
    if not verdict.satisfied:
        take_corrective_action(verdict)
```

**Key Theorem**: For any BF-MFOTL formula Ï† with temporal depth d and window size B, the streaming monitor uses at most O(B Ã— |Ï†|^d) memory regardless of trace length.

### 2. Categorical Safety Composition

**Problem**: Safety of composed AI systems is not guaranteed from component safety.

**Solution**: Category-theoretic framework where safety certificates compose mathematically.

```python
from aegis_omega import SafetyCategory, SafetyObject, SafePipeline

# Define safety category
category = SafetyCategory()

# Register AI systems with their safety specifications
preprocessor = SafetyObject(
    system=my_preprocessor,
    specification="âˆ€x. valid_input(x) â†’ valid_output(preprocess(x))"
)
model = SafetyObject(
    system=my_model,
    specification="âˆ€x. valid_input(x) â†’ safe_output(infer(x))"
)

# Compose with guaranteed safety preservation
pipeline = SafePipeline(category)
pipeline.add_stage(preprocessor)
pipeline.add_stage(model)

# Verify: pipeline safety follows from component safety
assert pipeline.verify()  # Mathematically proven!
```

**Key Theorem**: If f: (A, Ï†_A) â†’ (B, Ï†_B) and g: (B, Ï†_B) â†’ (C, Ï†_C) are safety-preserving morphisms, then g âˆ˜ f: (A, Ï†_A) â†’ (C, Ï†_C) is safety-preserving.

### 3. Folded-ZKML: Zero-Knowledge Safety Proofs

**Problem**: Verifying AI safety requires revealing model internals, creating security/IP risks.

**Solution**: Zero-knowledge proofs that AI outputs satisfy safety properties without revealing the model.

```python
from aegis_omega import FoldedZKMLProver, FoldedZKMLVerifier, SafetyProof

# Prover (AI provider) generates proof
prover = FoldedZKMLProver(model=my_llm)
proof = prover.generate_proof(
    input=user_query,
    output=model_response,
    safety_spec="no_harmful_content âˆ§ factually_grounded"
)

# Verifier (regulator/user) verifies without seeing model
verifier = FoldedZKMLVerifier()
assert verifier.verify(proof)  # Model complies, but internals hidden!
```

**Key Properties**:
- **Completeness**: If output satisfies spec, proof verifies
- **Soundness**: If proof verifies, output satisfies spec (with overwhelming probability)
- **Zero-Knowledge**: Verifier learns nothing about model parameters

### 4. AI Safety Handshake Protocol

**Problem**: AI systems cannot verify each other's safety properties before interaction.

**Solution**: Universal protocol for AI-to-AI safety negotiation, similar to TLS for encryption.

```python
from aegis_omega import AEGISOmega

# Initialize AEGIS-Î© on both AI systems
agent_a = AEGISOmega(system_id="agent_a")
agent_b = AEGISOmega(system_id="agent_b")

# Safety handshake before interaction
session = agent_a.initiate_handshake(agent_b)

# Phase 1: Capability Exchange
# Phase 2: Certificate Negotiation  
# Phase 3: Continuous Monitoring

if session.verified:
    # Safe to proceed with interaction
    result = agent_a.interact(agent_b, task)
```

## ðŸš€ Quick Start

### Installation

```bash
# Basic installation
pip install aegis-omega

# With all optional dependencies
pip install aegis-omega[all]

# Development installation
git clone https://github.com/shujaat-zaheer/aegis-omega
cd aegis-omega
pip install -e ".[dev]"
```

### Basic Usage

```python
from aegis_omega import create_aegis_for_eu_ai_act, AIAction

# Create AEGIS-Î© configured for EU AI Act compliance
aegis = create_aegis_for_eu_ai_act(enforcement_mode="strict")

# Process AI actions with automatic compliance checking
action = AIAction(
    action_id="act_001",
    action_type="model_inference",
    content={"input": user_query, "output": model_response},
    metadata={"model": "gpt-4", "timestamp": "2025-01-15T10:30:00Z"}
)

verdict = aegis.process_action(action)

if verdict.compliant:
    print(f"âœ“ Action compliant: {verdict.certificate}")
else:
    print(f"âœ— Violations: {verdict.violations}")
    print(f"  Required actions: {verdict.required_actions}")
```

## ðŸ“‹ EU AI Act Compliance

AEGIS-Î© provides complete formal specifications for EU AI Act high-risk requirements:

| Article | Requirement | MFOTL Specification | Status |
|---------|-------------|---------------------|--------|
| Art. 9 | Risk Management | `â–¡[0,âˆž)(risk_assessed â†’ monitored)` | âœ… Implemented |
| Art. 12 | Logging | `â–¡[0,1s](action â†’ logged)` | âœ… Implemented |
| Art. 13 | Transparency | `â–¡[0,100ms](output â†’ explained)` | âœ… Implemented |
| Art. 14 | Human Oversight | `â–¡[0,5min](high_risk â†’ human_notified)` | âœ… Implemented |
| Art. 15 | Accuracy/Robustness | `â–¡[0,1hr](accuracy_check)` | âœ… Implemented |

```python
from aegis_omega.mfotl import EUAIActSpecifications

# Get all Article 12 (logging) specifications
logging_specs = EUAIActSpecifications.get_article_12_specifications()

for spec in logging_specs:
    print(f"{spec.name}: {spec.formula}")
```

## ðŸ—ºï¸ Research Roadmap

### Phase 1: Foundations (Months 1-12)
- [ ] Streaming-MFOTL algorithm formalization
- [ ] OCaml prototype integrated with VeriMon
- [ ] Isabelle/HOL correctness proofs
- [ ] **Target**: RV 2027, CAV 2027

### Phase 2: Composition (Months 13-24)
- [ ] Categorical safety framework
- [ ] Certificate algebra implementation
- [ ] Coq formalization
- [ ] **Target**: POPL 2028, ICFP 2028

### Phase 3: Zero-Knowledge (Months 25-36)
- [ ] Folded-ZKML protocol
- [ ] MFOTL-to-R1CS compiler
- [ ] GPU-accelerated proving
- [ ] **Target**: IEEE S&P 2029, CCS 2029

### Phase 4: Protocol (Months 37-48)
- [ ] Safety Handshake specification
- [ ] Tamarin/ProVerif verification
- [ ] Industry pilots
- [ ] **Target**: USENIX Security 2030

## ðŸ“Š Benchmarks

| Component | Metric | Target | Current |
|-----------|--------|--------|---------|
| Streaming-MFOTL | Throughput | >10,000 events/sec | TBD |
| Streaming-MFOTL | Latency | <10ms | TBD |
| Streaming-MFOTL | Memory | <100MB | TBD |
| Folded-ZKML | Proof Generation | <5 min (7B model) | TBD |
| Folded-ZKML | Proof Size | <1MB | TBD |
| Folded-ZKML | Verification | <5s | TBD |
| Safety Handshake | Latency | <100ms | TBD |
| Safety Handshake | Overhead | <5% | TBD |

## ðŸ”¬ Research Foundation

This work builds on verified research:

1. **MFOTL & VeriMon**: Basin et al., ETH ZÃ¼rich (2008-2025)
   - 15,000+ lines verified Isabelle/HOL code
   - 11,000+ lines production OCaml (EnfGuard)

2. **zkLLM**: Sun et al., UC Berkeley (2024)
   - Zero-knowledge proofs for LLM inference
   - tlookup and zkAttn protocols

3. **Nova Folding**: Kothapalli et al., Carnegie Mellon (CRYPTO 2022)
   - Incrementally verifiable computation
   - O(1) recursive proof composition

4. **EU AI Act**: European Commission (2024)
   - High-risk AI requirements
   - Enforcement timeline: August 2025-2027

## ðŸ“– Documentation

- [API Reference](https://aegis-omega.readthedocs.io/en/latest/api/)
- [User Guide](https://aegis-omega.readthedocs.io/en/latest/guide/)
- [Research Paper](docs/AEGIS_Omega_Proposal.pdf)
- [Examples](examples/)

## ðŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

Areas where help is especially welcome:
- Isabelle/HOL proof mechanization
- GPU kernel optimization for ZK proving
- Integration with ML frameworks (PyTorch, JAX)
- EU AI Act specification extensions

## ðŸ“„ License

Apache License 2.0. See [LICENSE](LICENSE) for details.

## ðŸ“¬ Contact

- **Author**: H M Shujaat Zaheer
- **Supervisor**: Prof. David Basin, ETH ZÃ¼rich
- **Institution**: Information Security Group, ETH ZÃ¼rich

## ðŸ™ Acknowledgments

This research is conducted at ETH ZÃ¼rich's Information Security Group, building on the MonPoly/VeriMon ecosystem developed over 15+ years. We thank the Tamarin team for protocol verification infrastructure and the broader formal methods community.

---

<p align="center">
  <b>AEGIS-Î©: Making AI Safety Universal, Verifiable, and Composable</b>
</p>
